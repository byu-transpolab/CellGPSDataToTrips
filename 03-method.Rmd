# Methods

```{r setup2, include = FALSE}
# load R libraries here; the `include` flag in the chunk options above tells
# whether to print the results or not. Usually you don't want to print the
# library statements, or any code on the pdf.

# Main Packages ========
# I use these in every doc
library(tidyverse)
library(knitr)
library(kableExtra)
library(modelsummary)

options(dplyr.summarise.inform = FALSE)

# Other packages ------
# These sometimes get used, and sometimes don't.
library(mlogit)

# Instructions and options =========
# prints missing data in tables as blank space
options(knitr.kable.NA = '') 
# tells kableExtra to not load latex table packages in the chunk output
options(kableExtra.latex.load_packages = FALSE) 
options(modelsummary_format_numeric_latex = "plain")

# round and format numbers that get printed in the text of the article.
inline_hook <- function(x) {
  if (is.numeric(x)) {
    format(x, digits = 3, big.mark = ",")
  } else x
}
knitr::knit_hooks$set(inline = inline_hook)

knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

# options for latex-only output
if(knitr::is_latex_output()) {
  knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
} 

```

## Data

The GPS data used to determine the minPts and eps parameters come from 60 volunteers in
the Utah County area and were taken over a period of six or more months depending on the person. An example of what the raw GPS data looked like is shown in Figure 2.1.

```{r Figure1, echo=FALSE, message=FALSE, warning=FALSE}
rawData <- read_csv("data/Health Study_5f5184e73e2fd848eac22aec_passivelocation_57.csv")

rawData <- head(rawData)

rawData1 <- rawData %>%
  select(accuracy, timestamp, speed, lat,lon, time)

rawData1 %>%
kbl(booktabs = T, caption = "Raw GPS Data", longtable = T) %>%
  kable_styling(full_width = F)

```

Before the GPS data can be processed, it had to be cleaned and reformatted. Since a DBSCAN algorithm is being used, the speed variable was removed completely. From there, the dates and times had to be reformatted using functions from the **lubridate** package in R and by writing a "Yesterday" function that defines days as being from 3 AM to 3 AM instead of 12 AM to 12 AM. This was done because large amount of the demographic is college students, and they are likely to make trips after midnight. Table 2.2 shows what the cleaned data looked like.

```{r Figure2, echo=FALSE, message=FALSE, warning=FALSE}
source("R/gps2trips.R")

cleaned_data <- makeCaps("data")
cleaned_data <- head(cleaned_data)

cleaned_data %>%
kbl(booktabs = T, caption = "Cleaned GPS Data", longtable = T) %>%
  kable_styling(full_width = F)

```


## Models

Once the data is cleaned and properly formatted, it is run through a DBSCAN algorithm largely based on the method done by Gong et al. in 2018 [@GongInspiration]. This is where the eps and MinPts parameters will be chosen. After the DBSCAN algorithm determines how many clusters there are, they get split based on the time between consecutive points. This is to remove any error that comes from when a person makes two trips to the same place in one day (Ex. Home). Now, that cluster counts as two separate clusters if the time difference between the clusters is large enough. 

Finally, there is an entropy calculation step where entropy is determined by the change in departure angle between consecutive points. The equation for this entropy is shown in the equation below [@GongInspiration]. If the points are in a line, the person is likely at a stoplight and the entropy is very low. Implementing this entropy equation also removes the need for any subjective time or speed rules. For the purposes of this question, the entropy constraint will always be set to 0.5. The only changing parameters will be MinPts and eps.

\begin{equation}
  EI_q = -\sum_{d=1}^D ((\frac{n_d}{N})ln(\frac{n_d}{N}))
\end{equation}

The resulting clusters from this DBSCAN and entropy hybrid model are displayed in maps using R. For this experiment, I will look at a map of the cleaned data and determine how many clusters it looks like there is. Then, the data goes through the model and the calculated number of clusters is compared to how many I saw originally. I repeated this 5 times over 10 days in order to determine which eps and MinPts have the lowest error.

