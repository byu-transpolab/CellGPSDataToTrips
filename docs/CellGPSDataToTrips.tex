\documentclass[3p, authoryear]{elsarticle} %review=doublespace preprint=single 5p=2 column
%%% Begin My package additions %%%%%%%%%%%%%%%%%%%
\usepackage[hyphens]{url}

  \journal{Submitted to Journal} % Sets Journal name


\usepackage{lineno} % add
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\usepackage{graphicx}
%%%%%%%%%%%%%%%% end my additions to header

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{fontspec}
  \ifxetex
    \usepackage{xltxtra,xunicode}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={Determining DBSCAN-Entrpopy Hybrid Algorithm Parameters for Converting GPS Points to Activities},
            colorlinks=false,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls

\setcounter{secnumdepth}{5}
% Pandoc toggle for numbering sections (defaults to be off)

% Pandoc citation processing

% Pandoc header
\usepackage{booktabs}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}



\begin{document}
\begin{frontmatter}

  \title{Determining DBSCAN-Entrpopy Hybrid Algorithm Parameters for Converting GPS Points to Activities}
    \author[Brigham Young University]{Gillian Riches\corref{1}}
   \ead{martingillian4@gmail.com} 
    \author[Brigham Young University]{Gregory Macfarlane\corref{2}}
   \ead{gregmacfarlane@byu.edu} 
      \address[Brigham Young University]{Civil and Environmental Engineering Department, 232 Engineering Building, Provo, Utah 84602}
      \cortext[1]{Corresponding Author}
    \cortext[2]{Present affiliation: Committee Chair}
  
  \begin{abstract}
  This is where the abstract should go.
  \end{abstract}
   \begin{keyword} GPS Data, Trips, Clusters\end{keyword}
 \end{frontmatter}

\hypertarget{question}{%
\section{Question}\label{question}}

Global Positioning System (GPS) surveys have become a more accurate and reputable alternative to previous travel survey methods that collect activity-travel patterns. Despite GPS devices' ability to record time and positional characteristics, they still require two steps, cleaning and processing, in order to convert the positional characteristics into trip purposes and activities.

Currently, many researchers use time and speed rule-based algorithms to define when and where activities occur \citep{reviewOfMethods2014}. Due to their subjective nature, these rules are not ideal. For example, people walk at different speeds, so the speed threshold at which someone is considered stagnant would require manual changing from person to person. If not changed, the number of activities for each person could be misleading and inaccurate. These issues may explain why rule-based algorithms' accuracies typically range from 43\% to 61\% \citep{reviewOfMethods2014}. While these processing results (Step 2) are not ideal, the data-cleaning method (Step 1) still serves as a reputable guide regardless of the processing method.

Therefore, once the data is cleaned with accordance to rule-based algorithm methods, a DBSCAN and entropy based algorithm should be applied for the processing step. In this type of algorithm, four parameters are needed to determine an activity: minimum number of points (minpts) within a predefined radius (eps) with a minimum amount of entropy (entr\_t) \citep{GongInspiration}, and after a certain period of time from the previous activity (delta\_t) . When selected properly, these parameters will not need to change from person to person and therefore often lead to more accurate activity counts. In fact, one experiment \citep{DBAlgorithm2017} using just a DBSCAN cluster-based algorithms proved to be 92\% precise.

One way to select the minPoints and radius (eps) thresholds is to arbitrarily pick the minPoints based on how large the data set is (with a minimum of three) and then set k = minPts in a k-distance plot \citep{RKNNMethod2018}. Good values of the radius value is where the k-distance plot shows a strong bend. Another method involves calculating the arithmetic mean and standard deviation of a synthetic GPS trajectory, and subject those values to a Gaussian curve equation to solve for eps given an arbitrary minPts \citep{GaussianMethod2009}. Unfortunately, these methods only work in a pure DBSCAN algorithm where only minpts and eps are accounted for, not entr\_t and delta\_t.

Hence, the purpose of this paper is to explore a method of how to simultaneously select all four parameters as accurately as possible in a DBSCAN entropy based algorithm after the GPS data has been cleaned.

\hypertarget{methods}{%
\section{Methods}\label{methods}}

\hypertarget{data}{%
\subsection{Data}\label{data}}

The GPS data used to determine the four most accurate parameters come from 60 volunteers in
the Utah County area and were taken over a period of six or more months depending on the person. An example of what the raw GPS data looked like is shown in Figure 2.1.

\begin{longtable}[t]{rlrrrr}
\caption{\label{tab:Figure1}Raw GPS Data}\\
\toprule
accuracy & timestamp & speed & lat & lon & time\\
\midrule
16 & 2021-03-17 22:59:36 & -1 & 40.25293 & -111.6602 & 1.616044e+12\\
16 & 2021-03-17 22:59:37 & -1 & 40.25293 & -111.6602 & 1.616044e+12\\
16 & 2021-03-17 22:59:38 & -1 & 40.25293 & -111.6602 & 1.616044e+12\\
16 & 2021-03-17 22:59:39 & -1 & 40.25293 & -111.6602 & 1.616044e+12\\
16 & 2021-03-17 22:59:40 & -1 & 40.25293 & -111.6602 & 1.616044e+12\\
\addlinespace
16 & 2021-03-17 22:59:41 & -1 & 40.25293 & -111.6602 & 1.616044e+12\\
\bottomrule
\end{longtable}

Before the GPS data can be processed, it had to be cleaned and reformatted. Since a DBSCAN algorithm is being used, the speed variable was removed completely. From there, the dates and times had to be reformatted using functions from the \textbf{lubridate} package in R and by writing a ``Yesterday'' function with output ``ActivityDay'' that defines activity days as being from 3 AM to 3 AM instead of 12 AM to 12 AM. This was done because many respondents are college students, so they are likely to make trips after midnight. Table 2.2 shows what the cleaned data looked like.

\begin{longtable}[t]{rrllll}
\caption{\label{tab:Figure2}Cleaned GPS Data}\\
\toprule
lat & lon & timestamp & date & time & activityDay\\
\midrule
40.25293 & -111.6602 & 2021-03-17 22:59:36 & 2021-03-17 & 22:59:36 & 17-3\\
40.25293 & -111.6602 & 2021-03-17 22:59:37 & 2021-03-17 & 22:59:37 & 17-3\\
40.25293 & -111.6602 & 2021-03-17 22:59:38 & 2021-03-17 & 22:59:38 & 17-3\\
40.25293 & -111.6602 & 2021-03-17 22:59:39 & 2021-03-17 & 22:59:39 & 17-3\\
40.25293 & -111.6602 & 2021-03-17 22:59:40 & 2021-03-17 & 22:59:40 & 17-3\\
\addlinespace
40.25293 & -111.6602 & 2021-03-17 22:59:41 & 2021-03-17 & 22:59:41 & 17-3\\
\bottomrule
\end{longtable}

\hypertarget{models}{%
\subsection{Models}\label{models}}

Once the data is cleaned and properly formatted, it is run through a DBSCAN-entropy hybrid algorithm largely based on the method created by Gong et al.~in 2018 \citep{GongInspiration}. After the DBSCAN algorithm determines how many total clusters there are based on the eps and minpts parameters, they get further split based on the delta\_t parameter if necessary. If the time difference between points at the same place is greater than delta\_t, then the points will be split into two separate clusters or activities.

Finally, there is an entropy calculation step where entropy is determined by the change in departure angle between consecutive points. The equation for this entropy is shown in the equation below \citep{GongInspiration}. If the points are in a line, the entropy is very low. In this algorithm, the entr\_t parameter determines at which entropy someone is actually likely to be moving and not just at a stoplight, etc.

\begin{equation}
  EI_q = -\sum_{d=1}^D ((\frac{n_d}{N})ln(\frac{n_d}{N}))
\end{equation}

For this experiment, I will look at a map of the unprocessed, cleaned data and determine with my eyes how many clusters there are. Then, the hybrid algorithm will calculate the number of clusters using randomized values for the four parameters. Based on all the previous research that has been discussed, the ranges for the possible parameters are as follows:

minpts: (3,10)
eps: (1,50)
delta\_t: (300, 1500) seconds
entr\_t: (0.5,3)

Then, I will compare the amount of clusters I saw to the number of clusters the hybrid algorithm calculated. This process was repeated 5 times over 10 different days in order to determine which parameters are the most accurate.

\hypertarget{findings}{%
\section{Findings}\label{findings}}

This section might be called ``Results'' instead of ``Applications,'' depending
on what it is that you are working on. But you'll probably say something like
``The initial model estimation results are given in Table \ref{tab:estimation-results}.''
That table is created with the \texttt{modelsummary()} package and function.

With those results presented, you can go into a discussion of what they mean.
first, discuss the actual results that are shown in the table, and then any
interesting or unintuitive observations.

\hypertarget{additional-analysis}{%
\subsection{Additional Analysis}\label{additional-analysis}}

Usually, it is good to use your model for something.

\begin{itemize}
\tightlist
\item
  Hypothetical policy analysis
\item
  Statistical validation effort
\item
  Equity or impact analysis
\end{itemize}

If the analysis is substantial, it might become its own top-level section.

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{section}{Acknowledgements}

\bibliography{book.bib}


\end{document}
